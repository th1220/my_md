## 2 文本处理的基本方法

#### 学习目标

- 了解什么是分词, 词性标注, 命名实体识别及其它们的作用.
- 掌握分词, 词性标注, 命名实体识别流行工具的使用方法.

### 1 什么是分词

- 分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符。分词过程就是找到这样分界符的过程.

- 举个例子:

- ```
  传智教育是一家上市公司，旗下有黑马程序员品牌。我是在黑马这里学习人工智能
  
  ['传智', '教育', '是', '一家', '上市公司', '，', '旗下', '有', '黑马', '程序员', '品牌', '。', '我', '是', '在', '黑马', '这里', '学习', '人工智能']
  ```

- 分词的作用:

  - 词作为语言语义理解的最小单元, 是人类理解文本语言的基础. 因此也是AI解决NLP领域高阶任务, 如自动问答, 机器翻译, 文本生成的重要基础环节.

- 流行中文分词工具jieba:

  - 愿景: “结巴”中文分词, 做最好的 Python 中文分词组件.

- jieba的特性:

  - 支持多种分词模式
    - 精确模式
    - 全模式
    - 搜索引擎模式
  - 支持中文繁体分词
  - 支持用户自定义词典

- jieba的安装:

```
pip install jieba
```

- jieba的使用:

> - 精确模式分词:
> - 试图将句子最精确地切开，适合文本分析.

```python
import jieba
content = "传智教育是一家上市公司，旗下有黑马程序员品牌。我是在黑马这里学习人工智能"
# 精确模型：试图将句子最精确地切开，适合文本分析。也属于默认模式
jieba.cut(content, cut_all=False)  # cut_all默认为False

# 将返回一个生成器对象
<generator object Tokenizer.cut at 0x7f8d9053e650>

# 若需直接返回列表内容, 使用jieba.lcut即可
jieba.lcut(content, cut_all=False)
['传智', '教育', '是', '一家', '上市公司', '，', '旗下', '有', '黑马', '程序员', '品牌', '。', '我', '是', '在', '黑马', '这里', '学习', '人工智能']
```

- 精确模式分词:
- 试图将句子最精确地切开，适合文本分析.

```python
# 若需直接返回列表内容, 使用jieba.lcut即可
jieba.lcut(content, cut_all=True)

['传', '智', '教育', '是', '一家', '上市', '上市公司', '公司', '', '', '旗下', '下有', '黑马', '程序', '程序员', '品牌', '', '', '我', '是', '在', '黑马', '这里', '学习', '人工', '人工智能', '智能']

# 注意1：人工智能全模型分成三个词
# 注意2：逗号和句号也给分成了词
```

- 搜索引擎模式分词:
- 在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

```python
import jieba
content = "传智教育是一家上市公司，旗下有黑马程序员品牌。我是在黑马这里学习人工智能"
jieba.cut_for_search(content)

# 将返回一个生成器对象
<generator object Tokenizer.cut_for_search at 0x7f8d90e5a550>

# 若需直接返回列表内容, 使用jieba.lcut_for_search即可
jieba.lcut_for_search(content)
['传智', '教育', '是', '一家', '上市', '公司', '上市公司', '，', '旗下', '有', '黑马', '程序', '程序员', '品牌', '。', '我', '是', '在', '黑马', '这里', '学习', '人工', '智能', '人工智能']

# 对'无线电'等较长词汇都进行了再次分词.
```

- 中文繁体分词:
- 针对中国香港, 台湾地区的繁体文本进行分词。

```python
import jieba
content = "煩惱即是菩提，我暫且不提"
jieba.lcut(content)
['煩惱', '即', '是', '菩提', '，', '我', '暫且', '不', '提']
```

- 使用用户自定义词典:
- 添加自定义词典后, jieba能够准确识别词典中出现的词汇，提升整体的识别准确率。
- 词典格式: 每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。
- 词典样式如下, 将该词典存为userdict.txt, 方便之后加载使用。

```
# 格式：word1 freq1 word_type1
黑马程序员 5 n
传智教育 6 n
人工智能 7 nz
学习 3
上市 3 
```

```python
import jieba

sentence = '传智教育是一家上市公司，旗下有黑马程序员品牌。我是在黑马这里学习人工智能'
# 1 没有使用用户自定义词典
mydata = jieba.lcut(sentence, cut_all=False)
print('mydata-->', mydata)

# 2 使用用户自定义词典
jieba.load_userdict("./userdict.txt")
mydata2 = jieba.lcut(sentence, cut_all=False)
print('mydata2-->', mydata2)

# 没有使用用户自定义词典的分词效果
mydata--> ['传智', '教育', '是', '一家', '上市公司', '，', '旗下', '有', '黑马', '程序员', '品牌', '。', '我', '是', '在', '黑马', '这里', '学习', '人工智能']

# 使用用户自定义词典的分词效果
mydata2--> ['传智教育', '是', '一家', '上市公司', '，', '旗下', '有', '黑马程序员', '品牌', '。', '我', '是', '在', '黑马', '这里', '学习', '人工智能']
```



### 2 什么是命名实体识别

- 命名实体: 通常我们将人名, 地名, 机构名等专有名词统称命名实体. 如: 周杰伦, 黑山县, 孔子学院, 24辊方钢矫直机.
- 顾名思义, 命名实体识别(Named Entity Recognition，简称NER)就是识别出一段文本中可能存在的命名实体.
- 举个例子:

```tex
鲁迅, 浙江绍兴人, 五四新文化运动的重要参与者, 代表作朝花夕拾.

==>

鲁迅(人名) / 浙江绍兴(地名)人 / 五四新文化运动(专有名词) / 重要参与者 / 代表作 / 朝花夕拾(专有名词)
```

- 命名实体识别的作用:
  - 同词汇一样, 命名实体也是人类理解文本的基础单元, 因此也是AI解决NLP领域高阶任务的重要基础环节.

### 3 什么是词性标注

- 词性: 语言中对词的一种分类方法，以语法特征为主要依据、兼顾词汇意义对词进行划分的结果, 常见的词性有14种, 如: 名词, 动词, 形容词等.

- 顾名思义, 词性标注(Part-Of-Speech tagging, 简称POS)就是标注出一段文本中每个词汇的词性.

- 举个例子:

- ```python
  我爱自然语言处理
  
  ==>
  
  我/rr, 爱/v, 自然语言/n, 处理/vn
  
  rr: 人称代词
  v: 动词
  n: 名词
  vn: 动名词
  ```

- 词性标注的作用:

  - 词性标注以分词为基础, 是对文本语言的另一个角度的理解, 因此也常常成为AI解决NLP领域高阶任务的重要基础环节.

- 使用jieba进行中文词性标注:

```python
import jieba.posseg as pseg
pseg.lcut("我爱北京天安门") 
[pair('我', 'r'), pair('爱', 'v'), pair('北京', 'ns'), pair('天安门', 'ns')]

# 结果返回一个装有pair元组的列表, 每个pair元组中分别是词汇及其对应的词性, 具体词性含义请参照[附录: jieba词性对照表]
```

### 4 小结

- 学习了什么是分词:
  - 分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符, 分词过程就是找到这样分界符的过程.
- 学习了分词的作用:
  - 词作为语言语义理解的最小单元, 是人类理解文本语言的基础. 因此也是AI解决NLP领域高阶任务, 如自动问答, 机器翻译, 文本生成的重要基础环节.
- 学习了流行中文分词工具jieba:
  - 支持多种分词模式: 精确模式, 全模式, 搜索引擎模式
  - 支持中文繁体分词
  - 支持用户自定义词典
- 学习了jieba工具的安装和分词使用.
- 学习了什么是命名实体识别:
  - 命名实体: 通常我们将人名, 地名, 机构名等专有名词统称命名实体. 如: 周杰伦, 黑山县, 孔子学院, 24辊方钢矫直机.
  - 顾名思义, 命名实体识别(Named Entity Recognition，简称NER)就是识别出一段文本中可能存在的命名实体.
- 命名实体识别的作用:
  - 同词汇一样, 命名实体也是人类理解文本的基础单元, 因此也是AI解决NLP领域高阶任务的重要基础环节.
- 学习了什么是词性标注:
  - 词性: 语言中对词的一种分类方法，以语法特征为主要依据、兼顾词汇意义对词进行划分的结果, 常见的词性有14种, 如: 名词, 动词, 形容词等.
  - 顾名思义, 词性标注(Part-Of-Speech tagging, 简称POS)就是标注出一段文本中每个词汇的词性.
- 学习了词性标注的作用:
  - 词性标注以分词为基础, 是对文本语言的另一个角度的理解, 因此也常常成为AI解决NLP领域高阶任务的重要基础环节.
- 学习了使用jieba进行词性标注.